{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax_stable(Z):\n",
    "    \"\"\"\n",
    "    Compute softmax values for each sets of scores in Z.\n",
    "    each ROW of Z is a set of scores.\n",
    "    \"\"\"\n",
    "    e_Z = np.exp(Z - np.max(Z, axis = 1, keepdims = True))\n",
    "    A = e_Z / e_Z.sum(axis = 1, keepdims = True)\n",
    "    return A\n",
    "\n",
    "def crossentropy_loss(Yhat, y):\n",
    "    \"\"\"\n",
    "    Yhat: a numpy array of shape (Npoints, nClasses) -- predicted output\n",
    "    y: a numpy array of shape (Npoints) -- ground truth.\n",
    "    NOTE: We donâ€™t need to use the one-hot vector here since most of\n",
    "    elements are zeros. When programming in numpy, in each row of Yhat, we\n",
    "    need to access to the corresponding index only.\n",
    "    \"\"\"\n",
    "    id0 = range(Yhat.shape[0])\n",
    "    return -np.mean(np.log(Yhat[id0, y]))\n",
    "\n",
    "def mlp_init(d0, d1, d2):\n",
    "    \"\"\" Initialize W1, b1, W2, b2\n",
    "    d0: dimension of input data\n",
    "    d1: number of hidden unit\n",
    "    d2: number of output unit = number of classes\n",
    "    \"\"\"\n",
    "    W1 = 0.01*np.random.randn(d0, d1)\n",
    "    b1 = np.zeros(d1)\n",
    "    W2 = 0.01*np.random.randn(d1, d2)\n",
    "    b2 = np.zeros(d2)\n",
    "    return (W1, b1, W2, b2)\n",
    "\n",
    "def mlp_predict(X, W1, b1, W2, b2):\n",
    "    \"\"\"Suppose the network has been trained, predict class of new points.\n",
    "    X: data matrix, each ROW is one data point.\n",
    "    W1, b1, W2, b2: learned weight matrices and biases\n",
    "    \"\"\"\n",
    "    Z1 = X @ W1 + b1 # shape (N, d1)\n",
    "    A1 = np.maximum(Z1, 0) # shape (N, d1) -- ReLU\n",
    "    Z2 = A1 @ W2 + b2 # shape (N, d2)\n",
    "    return np.argmax(Z2, axis=1)\n",
    "\n",
    "def mlp_fit(X, y, W1, b1, W2, b2, eta):\n",
    "    loss_hist = []\n",
    "    for i in range(20000): # number of epochs\n",
    "        # feedforward\n",
    "        Z1 = X @ W1 + b1 # shape (N, d1)\n",
    "        A1 = np.maximum(Z1, 0) # shape (N, d1)\n",
    "        Z2 = A1 @ W2 + b2 # shape (N, d2)\n",
    "        Yhat = softmax_stable(Z2) # shape (N, d2)\n",
    "        if i %1000 == 0: # print loss after each 1000 iterations\n",
    "            loss = crossentropy_loss(Yhat, y)\n",
    "            print(\"iter %d, loss: %f\" %(i, loss))\n",
    "            loss_hist.append(loss)\n",
    "            \n",
    "        # back propagation\n",
    "        id0 = range(Yhat.shape[0])\n",
    "        Yhat[id0, y] -=1\n",
    "        E2 = Yhat/N # shape (N, d2)\n",
    "        dW2 = A1.T @ E2 # shape (d1, d2)\n",
    "        db2 = np.sum(E2, axis = 0) # shape (d2,)\n",
    "        E1 = E2 @ W2.T # shape (N, d1)\n",
    "        E1[Z1 <= 0] = 0 # gradient of ReLU, shape (N, d1)\n",
    "        dW1 = X.T @ E1 # shape (d0, d1)\n",
    "        db1 = np.sum(E1, axis = 0) # shape (d1,)\n",
    "        \n",
    "        # Gradient Descent update\n",
    "        W1 += -eta*dW1\n",
    "        b1 += -eta*db1\n",
    "        W2 += -eta*dW2\n",
    "        b2 += -eta*db2\n",
    "        \n",
    "    return (W1, b1, W2, b2, loss_hist)\n",
    "\n",
    "# data ???\n",
    "\n",
    "# suppose X, y are training input and output, respectively\n",
    "d0 = 2 # data dimension\n",
    "d1 = h = 100 # number of hidden units\n",
    "d2 = C = 3 # number of classes\n",
    "eta = 1 # learning rate\n",
    "(W1, b1, W2, b2) = mlp_init(d0, d1, d2)\n",
    "(W1, b1, W2, b2, loss_hist) = mlp_fit(X, y, W1, b1, W2, b2, eta)\n",
    "y_pred = mlp_predict(X, W1, b1, W2, b2)\n",
    "acc = 100*np.mean(y_pred == y)\n",
    "print('training accuracy: %.2f %%' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 91.67 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nhay103\\appdata\\local\\programs\\python\\python38-32\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:471: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "means = [[-1, -1], [1, -1], [0, 1]]\n",
    "cov = [[1, 0], [0, 1]]\n",
    "N = 20\n",
    "X0 = np.random.multivariate_normal(means[0], cov, N)\n",
    "X1 = np.random.multivariate_normal(means[1], cov, N)\n",
    "X2 = np.random.multivariate_normal(means[2], cov, N)\n",
    "X = np.concatenate((X0, X1, X2), axis = 0)\n",
    "y = np.asarray([0]*N + [1]*N + [2]*N)\n",
    "\n",
    "alpha = 1e-1 # regularization parameter\n",
    "clf = MLPClassifier(solver='lbfgs', alpha=alpha, hidden_layer_sizes=(100))\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(X)\n",
    "acc = 100*np.mean(y_pred == y)\n",
    "print('training accuracy: %.2f %%' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
